---
title: "&#32;"
output: html_document
params:
  results: null
---

```{r}
#| name: "results"
#| include: false
#| setup: true

library(tidyverse)
library(plotly)

res <- params$results
```
## Pmetrics Run Comparison {.tabset .tabset-fade}


### Likelihood 
```{r}
#| name: "likelihood"
#| echo: false
#| results: 'asis'


res$p3

# annoying workaround for knitr not rendering properly when combined in a single if statement

if(!is.null(res$ft1)){
  cat("**Note:** Final cycle gamma/lambda are not the same for all runs. Comparison by likelihood is not valid if these differ, as shown below.\n")
  cat(glue::glue("Fix `initial` in your models with `err = {tolower(res$ft1$body$dataset$Type[1])}(initial, coeff, fixed = TRUE)`, . See `?PM_model` for details.\n"))
}

if(!is.null(res$ft1)){
  res$ft1
}

if(!is.null(res$ft1)){
  cat(glue::glue("For models with the same final gamma/lambda, likelihood can be compared across runs. Models with lower values are more likely. {toupper(getPMoptions('ic_method'))} is a function of the likelihood, penalized for the number of parameters in the model."))
} else {
  cat(c("**Note:**", glue::glue("Models with lower values are more likely. {toupper(getPMoptions('ic_method'))} is a function of the likelihood, penalized for the number of parameters in the model.")))
}
```


### Prediction Errors 
```{r}
#| name: "prediction_errors"
#| echo: false

res$p1

```
**Bias** is `r res$metric_types$bias` $pred - obs$. **Imprecision** is `r res$metric_types$imprecision`. Bias is the inverse of accuracy and imprecision is the mean variability in bias (scatter) across all predictions. **Ideal for both bias and imprecision is 0**, i.e., models closest to the horizontal 0 line are most acccurate and precise. Change the bias and imprecision metrics with `setPMoptions()`.

### Obs vs Pred Plots
```{r}
#| name: "obs_vs_pred"
#| echo: false
#| fig.width: 10
#| fig.height: 10

res$p2

```


### Model Parameters
```{r}
#| name: "model_parameters"
#| echo: false
#| ft.align: "left"
#| ft.width: 100%

res$ft2

```
* ❌ = parameters not present in a model. 
* ☑️ = parameters present but not shared with the model in Run 1. 
* ✅ = parameters present and shared with the model in Run 1. They are used to calculate the P-value for the distribution of support point values comared to the first run. P-values <0.05 suggest a sigificantly different distribution of the joint probability value distribution across shared parameters with green check boxes using a k-nearest neighbors test. For a given model *Mx* in row *x*, P <0.05 means that the change from *M1* (row 1 model) to *Mx* resulted in a signficant change in values of parameters shared between *Mx* and *M1*. If other metrics are similar between models and P >0.05, the models may be considered equivalent